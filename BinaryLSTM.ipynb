{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-02T21:36:27.741082914Z",
     "start_time": "2023-06-02T21:36:23.442388753Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 00:36:24.497977: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-03 00:36:24.713121: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-03 00:36:24.716571: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-03 00:36:26.063832: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, GRU, Input\n",
    "from keras.models import Sequential\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as p\n",
    "import tensorflow as tensorflow\n",
    "from tensorflow.keras.models import load_model\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer as token\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "EMBEDDING_VECTOR_LENGTH = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    TOP_K = 20000\n",
    "    MAX_SEQUENCE_LENGTH = 33\n",
    "\n",
    "    def __init__(self, train_texts):\n",
    "        # токенизатор собственно\n",
    "        #\n",
    "        self.train_texts = train_texts\n",
    "        self.tokenizer = token(num_words=self.TOP_K)\n",
    "\n",
    "    def train_tokenize(self):\n",
    "        #\n",
    "        #\n",
    "        max_length = len(max(self.train_texts, key=len))\n",
    "        self.max_length = min(max_length, self.MAX_SEQUENCE_LENGTH)\n",
    "        self.tokenizer.fit_on_texts(self.train_texts)\n",
    "\n",
    "    def vectorize_input(self, tweets):\n",
    "        #\n",
    "        #\n",
    "        tweets = self.tokenizer.texts_to_sequences(tweets)\n",
    "        tweets = pad_sequences(tweets, maxlen=self.max_length, truncating='post', padding='post')\n",
    "        return tweets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T21:36:27.749033941Z",
     "start_time": "2023-06-02T21:36:27.742660039Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "SELECT_HI = str('(select text, hi FROM train_sets.all_set_hi WHERE (hi=1) ORDER BY random() LIMIT 3000)' +\n",
    "                'union all' +\n",
    "                '(select text, hi FROM train_sets.all_set_none WHERE (hi=0) ORDER BY random() LIMIT 1000)' +\n",
    "                'union all' +\n",
    "                '(select text, hi FROM train_sets.all_set_thanks WHERE (hi=0) ORDER BY random() LIMIT 500)' +\n",
    "                'union all' +\n",
    "                '(select text, hi FROM train_sets.all_set_business WHERE (hi=0) ORDER BY random() LIMIT 500)' +\n",
    "                'union all' +\n",
    "                '(select text, hi FROM train_sets.all_set_weather WHERE (hi=0) ORDER BY random() LIMIT 500)'\n",
    "                'union all' +\n",
    "                '(select text, hi FROM train_sets.all_set_trash WHERE (hi=0) ORDER BY random() LIMIT 500)')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T21:36:27.764081308Z",
     "start_time": "2023-06-02T21:36:27.749932177Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "conn_remote = psycopg2.connect(\n",
    "    'postgres://postgres:gaTResKPJX25@ep-round-paper-091468.us-east-2.aws.neon.tech/SistersMemory')\n",
    "\n",
    "target = 'hi'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T21:36:31.012772271Z",
     "start_time": "2023-06-02T21:36:27.759686312Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3217/3332240330.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  train = pd.read_sql(SELECT_HI, conn_remote)\n"
     ]
    },
    {
     "data": {
      "text/plain": "0                      дороу\n1                      дороу\n2                      жарко\n3                     прифок\n4                      жарко\n                ...         \n4881    поймать очень больно\n4882                    сука\n4883                  сучища\n4884                 спермер\n4885             сучка ебать\nName: text, Length: 4886, dtype: object"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_sql(SELECT_HI, conn_remote)\n",
    "train.text = train.text.astype(str)\n",
    "train.text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T21:36:32.375833600Z",
     "start_time": "2023-06-02T21:36:31.016330630Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  text  hi\n",
      "0                дороу   1\n",
      "2                жарко   1\n",
      "3               прифок   1\n",
      "6               привет   1\n",
      "7             здаровка   1\n",
      "...                ...  ..\n",
      "4866  ебать заваль бля   0\n",
      "4871               хуй   0\n",
      "4872             сучар   0\n",
      "4877       вяк сторона   0\n",
      "4885       сучка ебать   0\n",
      "\n",
      "[1548 rows x 2 columns]\n",
      "Shape of train (1238, 2)\n",
      "Shape of Validation  (310, 2)\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([train])\n",
    "train = df[~df[target].isna()]\n",
    "train[target] = train[target].astype(int)\n",
    "train = train.drop_duplicates()\n",
    "\n",
    "print(train)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train, train[target], test_size=0.2, random_state=64)\n",
    "print('Shape of train', X_train.shape)\n",
    "print('Shape of Validation ', X_val.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T21:36:32.391977557Z",
     "start_time": "2023-06-02T21:36:32.376316382Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Validation  (310, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Validation ', X_val.shape)\n",
    "tokenizer = Tokenizer(train_texts=X_train['text'])\n",
    "tokenizer.train_tokenize()\n",
    "tokenized_X_train = tokenizer.vectorize_input(X_train['text'])\n",
    "tokenized_X_val = tokenizer.vectorize_input(X_val['text'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T21:36:32.459741263Z",
     "start_time": "2023-06-02T21:36:32.392635991Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "optimzer = Adam(clipvalue=0.5)\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.tokenizer.word_index) + 1,\n",
    "                    EMBEDDING_VECTOR_LENGTH,\n",
    "                    input_length=tokenizer.MAX_SEQUENCE_LENGTH,\n",
    "                    trainable=True, mask_zero=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=optimzer, loss='binary_crossentropy',\n",
    "              metrics=['binary_accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T21:36:32.890374265Z",
     "start_time": "2023-06-02T21:36:32.463652553Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25/25 - 5s - loss: 0.3304 - binary_accuracy: 0.9273 - val_loss: 0.2412 - val_binary_accuracy: 0.9452 - 5s/epoch - 203ms/step\n",
      "Epoch 2/20\n",
      "25/25 - 1s - loss: 0.2348 - binary_accuracy: 0.9402 - val_loss: 0.2236 - val_binary_accuracy: 0.9452 - 957ms/epoch - 38ms/step\n",
      "Epoch 3/20\n",
      "25/25 - 1s - loss: 0.2151 - binary_accuracy: 0.9402 - val_loss: 0.1717 - val_binary_accuracy: 0.9452 - 921ms/epoch - 37ms/step\n",
      "Epoch 4/20\n",
      "25/25 - 1s - loss: 0.1508 - binary_accuracy: 0.9443 - val_loss: 0.1355 - val_binary_accuracy: 0.9548 - 914ms/epoch - 37ms/step\n",
      "Epoch 5/20\n",
      "25/25 - 1s - loss: 0.1090 - binary_accuracy: 0.9564 - val_loss: 0.1107 - val_binary_accuracy: 0.9613 - 986ms/epoch - 39ms/step\n",
      "Epoch 6/20\n",
      "25/25 - 1s - loss: 0.0806 - binary_accuracy: 0.9701 - val_loss: 0.1004 - val_binary_accuracy: 0.9742 - 1s/epoch - 41ms/step\n",
      "Epoch 7/20\n",
      "25/25 - 1s - loss: 0.0756 - binary_accuracy: 0.9742 - val_loss: 0.0942 - val_binary_accuracy: 0.9774 - 1s/epoch - 41ms/step\n",
      "Epoch 8/20\n",
      "25/25 - 1s - loss: 0.0633 - binary_accuracy: 0.9814 - val_loss: 0.0849 - val_binary_accuracy: 0.9774 - 942ms/epoch - 38ms/step\n",
      "Epoch 9/20\n",
      "25/25 - 1s - loss: 0.0664 - binary_accuracy: 0.9830 - val_loss: 0.1295 - val_binary_accuracy: 0.9774 - 993ms/epoch - 40ms/step\n",
      "Epoch 10/20\n",
      "25/25 - 1s - loss: 0.0517 - binary_accuracy: 0.9855 - val_loss: 0.1257 - val_binary_accuracy: 0.9774 - 1s/epoch - 41ms/step\n",
      "Epoch 11/20\n",
      "25/25 - 1s - loss: 0.0518 - binary_accuracy: 0.9911 - val_loss: 0.1144 - val_binary_accuracy: 0.9774 - 1s/epoch - 43ms/step\n",
      "Epoch 12/20\n",
      "25/25 - 1s - loss: 0.0509 - binary_accuracy: 0.9895 - val_loss: 0.1278 - val_binary_accuracy: 0.9774 - 974ms/epoch - 39ms/step\n",
      "Epoch 13/20\n",
      "25/25 - 1s - loss: 0.0426 - binary_accuracy: 0.9935 - val_loss: 0.1051 - val_binary_accuracy: 0.9774 - 932ms/epoch - 37ms/step\n",
      "Epoch 14/20\n",
      "25/25 - 1s - loss: 0.0448 - binary_accuracy: 0.9871 - val_loss: 0.1037 - val_binary_accuracy: 0.9774 - 1s/epoch - 44ms/step\n",
      "Epoch 15/20\n",
      "25/25 - 1s - loss: 0.0370 - binary_accuracy: 0.9927 - val_loss: 0.1016 - val_binary_accuracy: 0.9774 - 999ms/epoch - 40ms/step\n",
      "Epoch 16/20\n",
      "25/25 - 1s - loss: 0.0313 - binary_accuracy: 0.9952 - val_loss: 0.1269 - val_binary_accuracy: 0.9774 - 1s/epoch - 42ms/step\n",
      "Epoch 17/20\n",
      "25/25 - 1s - loss: 0.0262 - binary_accuracy: 0.9968 - val_loss: 0.1034 - val_binary_accuracy: 0.9774 - 995ms/epoch - 40ms/step\n",
      "Epoch 18/20\n",
      "25/25 - 1s - loss: 0.0403 - binary_accuracy: 0.9911 - val_loss: 0.1098 - val_binary_accuracy: 0.9774 - 1s/epoch - 42ms/step\n",
      "Epoch 19/20\n",
      "25/25 - 1s - loss: 0.0312 - binary_accuracy: 0.9952 - val_loss: 0.1083 - val_binary_accuracy: 0.9774 - 1s/epoch - 40ms/step\n",
      "Epoch 20/20\n",
      "25/25 - 1s - loss: 0.0352 - binary_accuracy: 0.9927 - val_loss: 0.0931 - val_binary_accuracy: 0.9774 - 959ms/epoch - 38ms/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(tokenized_X_train, y_train,\n",
    "                    validation_data=(tokenized_X_val, y_val),\n",
    "                    batch_size=51,\n",
    "                    epochs=20,\n",
    "                    verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T21:36:57.069814551Z",
     "start_time": "2023-06-02T21:36:32.896047504Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.977419376373291\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Tokenizers/0_lstmhitokenizer.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(score)\n\u001B[1;32m      3\u001B[0m model\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mModels/0_lstmhimodel.h5\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mTokenizers/0_lstmhitokenizer.pickle\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handle:\n\u001B[1;32m      5\u001B[0m     p\u001B[38;5;241m.\u001B[39mdump(tokenizer, handle,\n\u001B[1;32m      6\u001B[0m            protocol\u001B[38;5;241m=\u001B[39mp\u001B[38;5;241m.\u001B[39mHIGHEST_PROTOCOL)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    277\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    278\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    279\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    280\u001B[0m     )\n\u001B[0;32m--> 282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'Tokenizers/0_lstmhitokenizer.pickle'"
     ]
    }
   ],
   "source": [
    "score = history.history['val_binary_accuracy'].pop()\n",
    "print(score)\n",
    "model.save('Models/0_lstmhimodel.h5')\n",
    "with open('Tokenizers/0_lstmhitokenizer.pickle', 'wb') as handle:\n",
    "    p.dump(tokenizer, handle,\n",
    "           protocol=p.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T21:36:57.657022886Z",
     "start_time": "2023-06-02T21:36:57.081521202Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from pymystem3 import Mystem\n",
    "import re\n",
    "## данные в базе леманатизированы\n",
    "class CommonPreprocessing:\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess_text(cls, text):\n",
    "        # предобработчик приводит слова в начальную форму. Данные в базе находятся в леманатированном состоянии перед подачей на токенизацию в предикт слово тоже леманатируется\n",
    "        #\n",
    "        try:\n",
    "            tokens = str(text)\n",
    "            tokens = Mystem().lemmatize(text.lower())\n",
    "            tokens = [token for token in tokens if token not in stopwords.words('russian')\n",
    "                      and token != ' '\n",
    "                      and token.strip() not in punctuation]\n",
    "            tokens = [\n",
    "                token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "            text = ' '.join(tokens).rstrip('\\n')\n",
    "            pattern3 = r'[\\d]'\n",
    "            pattern2 = '[.]'\n",
    "            text = re.sub(pattern3, '', text)\n",
    "            text = re.sub(pattern2, '', text)\n",
    "            text = re.sub('  ', ' ', text)\n",
    "            return text\n",
    "        except:\n",
    "            return 'The exception is in CommonPreprocessing.preprocess_text'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(inpt, tmap, model, tokenizer):\n",
    "#\n",
    "#\n",
    "    model = load_model(model)\n",
    "    inn = []\n",
    "    pr = CommonPreprocessing()\n",
    "    for i in inpt:\n",
    "        inn.append(pr.preprocess_text(i))\n",
    "\n",
    "    with open(tokenizer, 'rb') as handle:\n",
    "        tokenizer = p.load(handle)\n",
    "        tokenized_inpt = tokenizer.vectorize_input(inn)\n",
    "\n",
    "    score = model.predict(tokenized_inpt)\n",
    "    outpt = max(np.round(score).astype(int))\n",
    "    outscore = max(score)\n",
    "    return(tmap[outpt[0]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "HIMAPA = {0: 'Не приветствие', 1: 'Приветствие'}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "modelpath = next(Path().rglob('0_lstmhimodel.h5'))\n",
    "tokenizerpath = next(Path().rglob('0_lstmhitokenizer.pickle'))\n",
    "hi = predict(\"хай\",HIMAPA,modelpath, tokenizerpath)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hi"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
