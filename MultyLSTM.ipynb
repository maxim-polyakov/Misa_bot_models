{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 23:56:25.387207: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-02 23:56:25.429178: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-02 23:56:25.430349: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-02 23:56:26.148534: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pickle as p\n",
    "import tensorflow as tensorflow\n",
    "from tensorflow.keras.models import load_model\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer as token\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "EMBEDDING_VECTOR_LENGTH = 33"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T20:56:27.315658677Z",
     "start_time": "2023-06-02T20:56:25.197356024Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    TOP_K = 20000\n",
    "    MAX_SEQUENCE_LENGTH = 33\n",
    "\n",
    "    def __init__(self, train_texts):\n",
    "# токенизатор собственно\n",
    "#\n",
    "        self.train_texts = train_texts\n",
    "        self.tokenizer = token(num_words=self.TOP_K)\n",
    "\n",
    "    def train_tokenize(self):\n",
    "#\n",
    "#\n",
    "        max_length = len(max(self.train_texts, key=len))\n",
    "        self.max_length = min(max_length, self.MAX_SEQUENCE_LENGTH)\n",
    "        self.tokenizer.fit_on_texts(self.train_texts)\n",
    "\n",
    "    def vectorize_input(self, tweets):\n",
    "#\n",
    "#\n",
    "        tweets = self.tokenizer.texts_to_sequences(tweets)\n",
    "        tweets = pad_sequences(tweets, maxlen=self.max_length, truncating='post', padding='post')\n",
    "        return tweets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T20:56:27.316025764Z",
     "start_time": "2023-06-02T20:56:27.249893932Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SELECT_EMOTIONS = str('(select text, emotionid  FROM train_sets.all_set_weather ORDER BY random() LIMIT 3000) ' +\n",
    "                      'union all ' +\n",
    "                      '(select text, emotionid FROM train_sets.all_set_none ORDER BY random() LIMIT 600) ' +\n",
    "                      'union all ' +\n",
    "                      '(select text, emotionid FROM train_sets.all_set_thanks ORDER BY random() LIMIT 600) ' +\n",
    "                      'union all ' +\n",
    "                      '(select text, emotionid FROM train_sets.all_set_hi ORDER BY random() LIMIT 600) ' +\n",
    "                      'union all ' +\n",
    "                      '(select text, emotionid  FROM train_sets.all_set_business ORDER BY random() LIMIT 600) ' +\n",
    "                      'union all ' +\n",
    "                      '(select text, emotionid  FROM train_sets.all_set_trash ORDER BY random() LIMIT 600) ')\n",
    "\n",
    "conn_remote = psycopg2.connect(\n",
    "    'postgres://postgres:gaTResKPJX25@ep-round-paper-091468.us-east-2.aws.neon.tech/SistersMemory')\n",
    "\n",
    "target = 'emotionid'"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-06-02T20:56:27.259614846Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train = pd.read_sql(SELECT_EMOTIONS, conn_remote)\n",
    "train.text = train.text.astype(str)\n",
    "train.text"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.concat([train])\n",
    "train = df[~df[target].isna()]\n",
    "train[target] = train[target].astype(int)\n",
    "train = train.drop_duplicates()\n",
    "\n",
    "print(train)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train, train[target], test_size=0.2, random_state=64)\n",
    "print('Shape of train', X_train.shape)\n",
    "print('Shape of Validation ', X_val.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(train_texts=X_train['text'])\n",
    "tokenizer.train_tokenize()\n",
    "tokenized_X_train = tokenizer.vectorize_input(X_train['text'])\n",
    "tokenized_X_val = tokenizer.vectorize_input(X_val['text'])\n",
    "y_trainmatrix = tensorflow.keras.utils.to_categorical(y_train, 7)\n",
    "y_valmatrix = tensorflow.keras.utils.to_categorical(y_val, 7)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "optimzer = Adam(learning_rate=0.005)\n",
    "model.add(Embedding(len(tokenizer.tokenizer.word_index) + 1,\n",
    "                    EMBEDDING_VECTOR_LENGTH,\n",
    "                    input_length=tokenizer.MAX_SEQUENCE_LENGTH,\n",
    "                    trainable=True))\n",
    "model.add(Bidirectional(LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(optimizer=optimzer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model.fit(tokenized_X_train, y_trainmatrix,\n",
    "          batch_size=51, epochs=20,\n",
    "          validation_data=(tokenized_X_val, y_valmatrix),\n",
    "          verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save('models/multy/LSTM/0_lstmemotionsmodel.h5')\n",
    "with open('tokenizers/multy/LSTM/0_lstmemotionstokenizer.pickle', 'wb') as handle:\n",
    "    p.dump(tokenizer, handle, protocol=p.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from pymystem3 import Mystem\n",
    "import re\n",
    "## данные в базе леманатизированы\n",
    "class CommonPreprocessing:\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess_text(cls, text):\n",
    "# предобработчик приводит слова в начальную форму. Данные в базе находятся в леманатированном состоянии перед подачей на токенизацию в предикт слово тоже леманатируется\n",
    "#\n",
    "        try:\n",
    "            tokens = str(text)\n",
    "            tokens = Mystem().lemmatize(text.lower())\n",
    "            tokens = [token for token in tokens if token not in stopwords.words('russian')\n",
    "                      and token != ' '\n",
    "                      and token.strip() not in punctuation]\n",
    "            tokens = [\n",
    "                token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "            text = ' '.join(tokens).rstrip('\\n')\n",
    "            pattern3 = r'[\\d]'\n",
    "            pattern2 = '[.]'\n",
    "            text = re.sub(pattern3, '', text)\n",
    "            text = re.sub(pattern2, '', text)\n",
    "            text = re.sub('  ', ' ', text)\n",
    "            return text\n",
    "        except:\n",
    "            return 'The exception is in CommonPreprocessing.preprocess_text'"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(inpt, tmap, model, tokenizer):\n",
    "#\n",
    "#\n",
    "    try:\n",
    "        model = load_model(model)\n",
    "        inn = []\n",
    "        pr = CommonPreprocessing()\n",
    "        for i in inpt:\n",
    "            inn.append(pr.preprocess_text(i))\n",
    "\n",
    "        with open(tokenizer, 'rb') as handle:\n",
    "            tokenizer = p.load(handle)\n",
    "            tokenized_inpt = tokenizer.vectorize_input(inn)\n",
    "\n",
    "        scoreplu = model.predict(tokenized_inpt)\n",
    "        outpt = tmap[scoreplu.argmax(axis=-1)[0]]\n",
    "        return outpt\n",
    "    except:\n",
    "        return 'The exeption is in MultyLSTM.predict'"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EMOTIONSMAPA = {0: '😞', 1: '🤬', 2: '😨', 3: '😊', 4: '❤', 5: '😳', 6: ''}"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 861ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "'😨'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelpath = 'models/multy/LSTM/0_lstmemotionsmodel.h5'\n",
    "tokenizerpath = 'tokenizers/multy/LSTM/0_lstmemotionstokenizer.pickle'\n",
    "emotion = predict('привет',EMOTIONSMAPA, modelpath, tokenizerpath)\n",
    "emotion"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T16:28:13.906602363Z",
     "start_time": "2023-06-02T16:28:09.096293683Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
