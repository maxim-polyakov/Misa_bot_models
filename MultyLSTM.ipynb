{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pickle as p\n",
    "import tensorflow as tensorflow\n",
    "from tensorflow.keras.models import load_model\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer as token\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "EMBEDDING_VECTOR_LENGTH = 33"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T12:07:28.636468190Z",
     "start_time": "2023-06-04T12:07:28.538317517Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    TOP_K = 20000\n",
    "    MAX_SEQUENCE_LENGTH = 33\n",
    "\n",
    "    def __init__(self, train_texts):\n",
    "# —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ\n",
    "#\n",
    "        self.train_texts = train_texts\n",
    "        self.tokenizer = token(num_words=self.TOP_K)\n",
    "\n",
    "    def train_tokenize(self):\n",
    "#\n",
    "#\n",
    "        max_length = len(max(self.train_texts, key=len))\n",
    "        self.max_length = min(max_length, self.MAX_SEQUENCE_LENGTH)\n",
    "        self.tokenizer.fit_on_texts(self.train_texts)\n",
    "\n",
    "    def vectorize_input(self, tweets):\n",
    "#\n",
    "#\n",
    "        tweets = self.tokenizer.texts_to_sequences(tweets)\n",
    "        tweets = pad_sequences(tweets, maxlen=self.max_length, truncating='post', padding='post')\n",
    "        return tweets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T12:07:28.682612128Z",
     "start_time": "2023-06-04T12:07:28.591232682Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "SELECT_EMOTIONS = str('(select text, emotionid  FROM train_sets.all_set_weather ORDER BY random() LIMIT 3000) ' +\n",
    "                      'union all ' +\n",
    "                      '(select text, emotionid FROM train_sets.all_set_none ORDER BY random() LIMIT 600) ' +\n",
    "                      'union all ' +\n",
    "                      '(select text, emotionid FROM train_sets.all_set_thanks ORDER BY random() LIMIT 600) ' +\n",
    "                      'union all ' +\n",
    "                      '(select text, emotionid FROM train_sets.all_set_hi ORDER BY random() LIMIT 600) ' +\n",
    "                      'union all ' +\n",
    "                      '(select text, emotionid  FROM train_sets.all_set_business ORDER BY random() LIMIT 600) ' +\n",
    "                      'union all ' +\n",
    "                      '(select text, emotionid  FROM train_sets.all_set_trash ORDER BY random() LIMIT 600) ')\n",
    "\n",
    "conn_remote = psycopg2.connect(\n",
    "    'postgres://postgres:gaTResKPJX25@ep-round-paper-091468.us-east-2.aws.neon.tech/SistersMemory')\n",
    "\n",
    "target = 'emotionid'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T12:07:29.640093028Z",
     "start_time": "2023-06-04T12:07:28.591559580Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21679/178288445.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  train = pd.read_sql(SELECT_EMOTIONS, conn_remote)\n"
     ]
    },
    {
     "data": {
      "text/plain": "0                                               –ø—Ä–æ—Ö–ª–∞–¥–Ω–æ\n1          —è—Å–Ω–æ –æ—Å–∞–¥–∫–∏ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–∞–≤–ª–µ–Ω–∏–µ–≤–µ—Ç–µ—Ä –∑–∞–ø–∞–¥–Ω—ã–π\n2       –ø–ª–æ—Ö–æ–π –ø–æ–≥–æ–¥–∞ –±—ã–≤–∞—Ç—å –±—ã–≤–∞—Ç—å –Ω–µ–ø–æ–¥—Ö–æ–¥—è—â–∏–π –∫–æ—Å—Ç—é...\n3       –º–µ—Ç–µ–æ—Ä–æ–ª–æ–≥–∏—è –Ω–∞—É—á–Ω—ã–π –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –Ω–µ–≤–µ—Ä–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑\n4       –º–∏–Ω–Ω–µ—Å–æ—Ç–∞ –±—ã–≤–∞—Ç—å —Ö–æ–ª–æ–¥–Ω—ã–π –Ω–æ—á—å –ø—Ä–∏—Ö–æ–¥–∏—Ç—å—Å—è –Ω–∞–¥...\n                              ...                        \n5712                               –≤—ã–µ–±—ã–≤–∞–µ—à—Å—è –µ–±–∞–Ω—ã–π —Ä–æ—Ç\n5713                                             —Ö–µ—Ä–æ–≤–∏–Ω–∞\n5714               —Ö—É–π –∫—É—à–∞—Ç—å –±–ª—è—Ç—å —á–µ –∑–≤–æ–Ω–∏—Ç—å —á–µ–±—É—Ä–µ—á–Ω—ã–π\n5715                                        —Å–ø–∏–¥–æ–∑–Ω—ã–π –ø–µ—Å\n5716                                            –∏–∑–≥–æ–≤–Ω—è—Ç—å\nName: text, Length: 5717, dtype: object"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_sql(SELECT_EMOTIONS, conn_remote)\n",
    "train.text = train.text.astype(str)\n",
    "train.text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T12:07:31.308328388Z",
     "start_time": "2023-06-04T12:07:29.626598332Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  emotionid\n",
      "0                                             –ø—Ä–æ—Ö–ª–∞–¥–Ω–æ          6\n",
      "1        —è—Å–Ω–æ –æ—Å–∞–¥–∫–∏ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–∞–≤–ª–µ–Ω–∏–µ–≤–µ—Ç–µ—Ä –∑–∞–ø–∞–¥–Ω—ã–π          6\n",
      "2     –ø–ª–æ—Ö–æ–π –ø–æ–≥–æ–¥–∞ –±—ã–≤–∞—Ç—å –±—ã–≤–∞—Ç—å –Ω–µ–ø–æ–¥—Ö–æ–¥—è—â–∏–π –∫–æ—Å—Ç—é...          6\n",
      "3     –º–µ—Ç–µ–æ—Ä–æ–ª–æ–≥–∏—è –Ω–∞—É—á–Ω—ã–π –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –Ω–µ–≤–µ—Ä–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑          6\n",
      "4     –º–∏–Ω–Ω–µ—Å–æ—Ç–∞ –±—ã–≤–∞—Ç—å —Ö–æ–ª–æ–¥–Ω—ã–π –Ω–æ—á—å –ø—Ä–∏—Ö–æ–¥–∏—Ç—å—Å—è –Ω–∞–¥...          6\n",
      "...                                                 ...        ...\n",
      "5705                                   –µ–±–∞—Ç—å –∑–∞–≤–∞–ª—å –±–ª—è          1\n",
      "5707                            –∫–ª–∏–∑–º–∞ –∑–Ω–∞—Ç—å —Å–≤–æ–π –º–µ—Å—Ç–æ          1\n",
      "5708                             –±–∞–±–∞–π–∫–∞ –¥–µ—Ç—Å—Ç–≤–æ –ø—É–≥–∞—Ç—å          1\n",
      "5713                                           —Ö–µ—Ä–æ–≤–∏–Ω–∞          1\n",
      "5715                                      —Å–ø–∏–¥–æ–∑–Ω—ã–π –ø–µ—Å          1\n",
      "\n",
      "[1328 rows x 2 columns]\n",
      "Shape of train (1062, 2)\n",
      "Shape of Validation  (266, 2)\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([train])\n",
    "train = df[~df[target].isna()]\n",
    "train[target] = train[target].astype(int)\n",
    "train = train.drop_duplicates()\n",
    "\n",
    "print(train)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train, train[target], test_size=0.2, random_state=64)\n",
    "print('Shape of train', X_train.shape)\n",
    "print('Shape of Validation ', X_val.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T12:07:31.309274282Z",
     "start_time": "2023-06-04T12:07:31.239716646Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(train_texts=X_train['text'])\n",
    "tokenizer.train_tokenize()\n",
    "tokenized_X_train = tokenizer.vectorize_input(X_train['text'])\n",
    "tokenized_X_val = tokenizer.vectorize_input(X_val['text'])\n",
    "y_trainmatrix = tensorflow.keras.utils.to_categorical(y_train, 7)\n",
    "y_valmatrix = tensorflow.keras.utils.to_categorical(y_val, 7)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T12:07:31.309776808Z",
     "start_time": "2023-06-04T12:07:31.240061472Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "optimzer = Adam(learning_rate=0.005)\n",
    "model.add(Embedding(len(tokenizer.tokenizer.word_index) + 1,\n",
    "                    EMBEDDING_VECTOR_LENGTH,\n",
    "                    input_length=tokenizer.MAX_SEQUENCE_LENGTH,\n",
    "                    trainable=True))\n",
    "model.add(Bidirectional(LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(optimizer=optimzer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T12:07:32.413425304Z",
     "start_time": "2023-06-04T12:07:31.311527909Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21/21 - 32s - loss: 1.6052 - accuracy: 0.3945 - val_loss: 1.2392 - val_accuracy: 0.4286 - 32s/epoch - 2s/step\n",
      "Epoch 2/20\n",
      "21/21 - 15s - loss: 0.9877 - accuracy: 0.6262 - val_loss: 0.9198 - val_accuracy: 0.6992 - 15s/epoch - 722ms/step\n",
      "Epoch 3/20\n",
      "21/21 - 14s - loss: 0.6038 - accuracy: 0.8079 - val_loss: 0.9731 - val_accuracy: 0.7068 - 14s/epoch - 682ms/step\n",
      "Epoch 4/20\n",
      "21/21 - 13s - loss: 0.5512 - accuracy: 0.8220 - val_loss: 1.0219 - val_accuracy: 0.7068 - 13s/epoch - 606ms/step\n",
      "Epoch 5/20\n",
      "21/21 - 13s - loss: 0.3601 - accuracy: 0.8804 - val_loss: 1.7156 - val_accuracy: 0.6955 - 13s/epoch - 597ms/step\n",
      "Epoch 6/20\n",
      "21/21 - 13s - loss: 0.3420 - accuracy: 0.8898 - val_loss: 1.0488 - val_accuracy: 0.7594 - 13s/epoch - 617ms/step\n",
      "Epoch 7/20\n",
      "21/21 - 13s - loss: 0.2387 - accuracy: 0.9237 - val_loss: 1.2973 - val_accuracy: 0.7481 - 13s/epoch - 635ms/step\n",
      "Epoch 8/20\n",
      "21/21 - 15s - loss: 0.1803 - accuracy: 0.9473 - val_loss: 1.2151 - val_accuracy: 0.7632 - 15s/epoch - 719ms/step\n",
      "Epoch 9/20\n",
      "21/21 - 15s - loss: 0.1225 - accuracy: 0.9642 - val_loss: 1.4801 - val_accuracy: 0.7218 - 15s/epoch - 720ms/step\n",
      "Epoch 10/20\n",
      "21/21 - 16s - loss: 0.1356 - accuracy: 0.9661 - val_loss: 1.3307 - val_accuracy: 0.7444 - 16s/epoch - 754ms/step\n",
      "Epoch 11/20\n",
      "21/21 - 16s - loss: 0.0982 - accuracy: 0.9708 - val_loss: 1.4751 - val_accuracy: 0.7707 - 16s/epoch - 771ms/step\n",
      "Epoch 12/20\n",
      "21/21 - 16s - loss: 0.1009 - accuracy: 0.9661 - val_loss: 1.5002 - val_accuracy: 0.7481 - 16s/epoch - 785ms/step\n",
      "Epoch 13/20\n",
      "21/21 - 16s - loss: 0.0695 - accuracy: 0.9812 - val_loss: 1.4273 - val_accuracy: 0.7669 - 16s/epoch - 763ms/step\n",
      "Epoch 14/20\n",
      "21/21 - 16s - loss: 0.0737 - accuracy: 0.9821 - val_loss: 1.3828 - val_accuracy: 0.7820 - 16s/epoch - 770ms/step\n",
      "Epoch 15/20\n",
      "21/21 - 16s - loss: 0.0533 - accuracy: 0.9859 - val_loss: 1.4852 - val_accuracy: 0.7707 - 16s/epoch - 759ms/step\n",
      "Epoch 16/20\n",
      "21/21 - 16s - loss: 0.0659 - accuracy: 0.9821 - val_loss: 1.4035 - val_accuracy: 0.7895 - 16s/epoch - 770ms/step\n",
      "Epoch 17/20\n",
      "21/21 - 16s - loss: 0.0475 - accuracy: 0.9840 - val_loss: 1.5034 - val_accuracy: 0.7594 - 16s/epoch - 768ms/step\n",
      "Epoch 18/20\n",
      "21/21 - 16s - loss: 0.0227 - accuracy: 0.9944 - val_loss: 1.5664 - val_accuracy: 0.7632 - 16s/epoch - 759ms/step\n",
      "Epoch 19/20\n",
      "21/21 - 17s - loss: 0.0483 - accuracy: 0.9878 - val_loss: 1.4822 - val_accuracy: 0.8008 - 17s/epoch - 790ms/step\n",
      "Epoch 20/20\n",
      "21/21 - 16s - loss: 0.0386 - accuracy: 0.9878 - val_loss: 1.5220 - val_accuracy: 0.7857 - 16s/epoch - 783ms/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(tokenized_X_train, y_trainmatrix,\n",
    "          batch_size=51, epochs=20,\n",
    "          validation_data=(tokenized_X_val, y_valmatrix),\n",
    "          verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T12:12:53.549188459Z",
     "start_time": "2023-06-04T12:07:32.423676767Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "model.save('models/multy/LSTM/lstmemotionsmodel.h5')\n",
    "with open('tokenizers/multy/LSTM/lstmemotionstokenizer.pickle', 'wb') as handle:\n",
    "    p.dump(tokenizer, handle, protocol=p.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T12:12:53.647249010Z",
     "start_time": "2023-06-04T12:12:53.521877620Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from pymystem3 import Mystem\n",
    "import re\n",
    "## –¥–∞–Ω–Ω—ã–µ –≤ –±–∞–∑–µ –ª–µ–º–∞–Ω–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω—ã\n",
    "class CommonPreprocessing:\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess_text(cls, text):\n",
    "#\n",
    "#\n",
    "        try:\n",
    "            tokens = str(text)\n",
    "            tokens = Mystem().lemmatize(text.lower())\n",
    "            tokens = [token for token in tokens if token not in stopwords.words('russian')\n",
    "                      and token != ' '\n",
    "                      and token.strip() not in punctuation]\n",
    "            tokens = [\n",
    "                token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "            text = ' '.join(tokens).rstrip('\\n')\n",
    "            pattern3 = r'[\\d]'\n",
    "            pattern2 = '[.]'\n",
    "            text = re.sub(pattern3, '', text)\n",
    "            text = re.sub(pattern2, '', text)\n",
    "            text = re.sub('  ', ' ', text)\n",
    "            return text\n",
    "        except:\n",
    "            return 'The exception is in CommonPreprocessing.preprocess_text'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T12:12:54.122955482Z",
     "start_time": "2023-06-04T12:12:53.640298587Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def predict(inpt, tmap, model, tokenizer):\n",
    "#\n",
    "#\n",
    "    try:\n",
    "        model = load_model(model)\n",
    "        inn = []\n",
    "        pr = CommonPreprocessing()\n",
    "        for i in inpt:\n",
    "            inn.append(pr.preprocess_text(i))\n",
    "\n",
    "        with open(tokenizer, 'rb') as handle:\n",
    "            tokenizer = p.load(handle)\n",
    "            tokenized_inpt = tokenizer.vectorize_input(inn)\n",
    "\n",
    "        scoreplu = model.predict(tokenized_inpt)\n",
    "        outpt = tmap[scoreplu.argmax(axis=-1)[0]]\n",
    "        return outpt\n",
    "    except:\n",
    "        return 'The exeption is in MultyLSTM.predict'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T12:12:54.131241976Z",
     "start_time": "2023-06-04T12:12:54.123667046Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "EMOTIONSMAPA = {0: 'üòû', 1: 'ü§¨', 2: 'üò®', 3: 'üòä', 4: '‚ù§', 5: 'üò≥', 6: ''}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T12:12:54.139016776Z",
     "start_time": "2023-06-04T12:12:54.130069621Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "'ü§¨'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelpath = 'models/multy/LSTM/lstmemotionsmodel.h5'\n",
    "tokenizerpath = 'tokenizers/multy/LSTM/lstmemotionstokenizer.pickle'\n",
    "emotion = predict('–ø—Ä–∏–≤–µ—Ç',EMOTIONSMAPA, wmodelpath, tokenizerpath)\n",
    "emotion"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T12:13:00.972841746Z",
     "start_time": "2023-06-04T12:12:54.137103709Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "w"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T12:13:00.982580997Z",
     "start_time": "2023-06-04T12:13:00.973817314Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
